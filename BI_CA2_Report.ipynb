{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac3f1a8b-c1ad-4703-830f-3b2e62d42658",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary modules\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "from datetime import timedelta\n",
    "import os\n",
    "from scipy import stats\n",
    "from sqlalchemy import create_engine\n",
    "import mysql.connector\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b6d640-783d-4715-9b42-a0aa2974b0a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3677c5e7-284d-4fce-b66e-722154043c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Importing datasets\n",
    "\n",
    "#####1 Customers Table\n",
    "\n",
    "#importing dataset\n",
    "customers = DataFrame()\n",
    "customers = pd.read_csv(r\"C:\\Users\\yasin\\OneDrive\\Desktop\\BI\\CA2\\Raw data\\Customers_Dim.csv\")\n",
    "\n",
    "#dropping unwanted columns\n",
    "customers = customers.drop(columns=['CustomerLabel', 'Title', 'MiddleName', 'NameStyle', 'BirthDate', 'MaritalStatus', 'Suffix', 'YearlyIncome', 'BirthDate', 'TotalChildren', 'NumberChildrenAtHome', 'HouseOwnerFlag', 'NumberCarsOwned', 'AddressLine2', 'DateFirstPurchase', 'CustomerType', 'CompanyName', 'ETLLoadID', 'LoadDate', 'UpdateDate'])\n",
    "\n",
    "#renaming \"CustomerKey to \"CustomerID\"\n",
    "customers = customers.rename(columns={\"CustomerKey\": \"CustomerID\", \"AddressLine1\": \"Address\"})\n",
    "\n",
    "#creating CustomerName by combining FirstName and LastName\n",
    "customers['CustomerName'] = customers['FirstName'] + ' ' + customers['LastName']\n",
    "\n",
    "#dropping the original FirstName and LastName columns\n",
    "customers = customers.drop(['FirstName', 'LastName'], axis=1)\n",
    "\n",
    "#moving 'CustomerName' to the 2nd position and 'GeographyKey' to the 7th\n",
    "cols = customers.columns.tolist()\n",
    "cols.insert(1, cols.pop(cols.index('CustomerName')))\n",
    "cols.insert(7, cols.pop(cols.index('GeographyKey')))\n",
    "customers = customers[cols]\n",
    "\n",
    "#replacing 'Gender' values for clarity\n",
    "customers['Gender'] = customers['Gender'].replace({\n",
    "    'F': 'Female',\n",
    "    'M': 'Male'\n",
    "})\n",
    "\n",
    "engine = create_engine('mysql+mysqlconnector://root:password@localhost/businessintelligence_warehouse')\n",
    "customers.to_sql(name='customers', con=engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deba7ca2-698d-42d9-99bd-9697d9c24d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47fa0ee9-59f1-469c-abd7-a4ed1f822c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####2 Products Table\n",
    "\n",
    "#importing dataset\n",
    "products = DataFrame()\n",
    "products = pd.read_csv(r\"C:\\Users\\yasin\\OneDrive\\Desktop\\BI\\CA2\\Raw data\\Products_Dim.csv\")\n",
    "\n",
    "#renaming \"CustomerKey to \"CustomerID\"\n",
    "products = products.rename(columns={\"ProductKey\": \"ProductID\"})\n",
    "\n",
    "engine = create_engine('mysql+mysqlconnector://root:password@localhost/businessintelligence_warehouse')\n",
    "products.to_sql(name='products', con=engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24ec80d-c372-4bce-9c07-2684a374af01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ada13cd0-1e15-46fc-b56b-f5e6fcb99898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged rows: 6000\n"
     ]
    }
   ],
   "source": [
    "#####3 Sales Table\n",
    "\n",
    "#importing dataset\n",
    "sales = pd.read_csv(r\"C:\\Users\\yasin\\OneDrive\\Desktop\\BI\\CA2\\Raw data\\Sales_Dim.csv\")\n",
    "\n",
    "#dropping rows where \"Quantity\" value = 0\n",
    "sales.drop(sales[sales['Quantity'] == 0].index, inplace=True)\n",
    "\n",
    "#dropping rows where \"Customer\" value = NaN \n",
    "sales = sales.dropna(subset=[\"CustomerName\"])\n",
    "\n",
    "#renaming \"SalesOrderNumber\" to \"SalesOrderID\"\n",
    "sales = sales.rename(columns={\"SalesOrderNumber\": \"SalesID\", \"CustomerKey\": \"CustomerID\"})\n",
    "\n",
    "#dropping rows where \"CustomerKey\" value <= 1000 in order to match the customers list in our Customers table\n",
    "sales.drop(sales[sales['CustomerID'] > 1000].index, inplace=True)\n",
    "\n",
    "#randomly sample 6000 rows from the Sales dataframe\n",
    "sales = sales.sample(n=6000, random_state=42)\n",
    "\n",
    "sales = sales.reset_index(drop=True)\n",
    "\n",
    "#calculating Sales DataFrame of 6000 rows\n",
    "total_rows = len(sales)\n",
    "\n",
    "#calculating number of rows per year for 6000 in total\n",
    "orders_in_2022 = int(total_rows * 0.39)\n",
    "orders_in_2023 = int(total_rows * 0.37)\n",
    "orders_in_2024 = total_rows - orders_in_2022 - orders_in_2023\n",
    "\n",
    "#generating random dates for each year\n",
    "dates_2022 = pd.to_datetime(np.random.choice(\n",
    "    pd.date_range(start='2022-01-01', end='2022-12-31'), size=orders_in_2022))\n",
    "\n",
    "dates_2023 = pd.to_datetime(np.random.choice(\n",
    "    pd.date_range(start='2023-01-01', end='2023-12-31'), size=orders_in_2023))\n",
    "\n",
    "dates_2024 = pd.to_datetime(np.random.choice(\n",
    "    pd.date_range(start='2024-01-01', end='2024-12-30'), size=orders_in_2024))\n",
    "\n",
    "#concatenating the dates\n",
    "all_dates = np.concatenate([dates_2022, dates_2023, dates_2024])\n",
    "np.random.shuffle(all_dates)\n",
    "\n",
    "#setting a new column for the dates and sorting by OrderDate\n",
    "sales['OrderDate'] = all_dates\n",
    "sales = sales.sort_values('OrderDate').reset_index(drop=True)\n",
    "\n",
    "#assigning a unique sales id for every order\n",
    "sales['SalesID'] = range(1, len(sales) + 1)\n",
    "\n",
    "#identifying unmatched sales rows\n",
    "#matched_sales = sales[sales['ProductName'].isin(products['ProductName'])]\n",
    "#unmatched_sales = sales[~sales['ProductName'].isin(products['ProductName'])]\n",
    "\n",
    "#adding a ProductID column matching the IDs from each ProductName in the products table \n",
    "sales = sales.merge(products[['ProductID', 'ProductName']], on='ProductName', how='inner')\n",
    "print(\"Merged rows:\", len(sales))\n",
    "\n",
    "#moving 'ProductID' new column to the 4th position\n",
    "cols = sales.columns.tolist()\n",
    "cols.insert(3, cols.pop(cols.index('ProductID')))\n",
    "sales = sales[cols]\n",
    "\n",
    "engine = create_engine('mysql+mysqlconnector://root:password@localhost/businessintelligence_warehouse')\n",
    "sales.to_sql(name='sales', con=engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1819e9b5-5320-451b-9f92-733620648263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae567a8a-0f10-4676-894b-a7247aa70758",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####4 Returns Table\n",
    "returns = pd.read_csv(r\"C:\\Users\\yasin\\OneDrive\\Desktop\\BI\\CA2\\Raw data\\Returns_Dim.csv\")\n",
    "#randomly selecting 1000 rows from the sales table to be used as returns table (using random_state)\n",
    "returns = sales.sample(n=1000, random_state=42)\n",
    "\n",
    "#adding ReturnID as the 1st column\n",
    "returns.insert(0, 'ReturnID', range(1, len(returns) + 1))\n",
    "\n",
    "engine = create_engine('mysql+mysqlconnector://root:password@localhost/businessintelligence_warehouse')\n",
    "returns.to_sql(name='returns', con=engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0104a6-24a7-4611-af75-e26b2d92d8b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a02543a0-65ad-4ec9-9ed6-dac1ac687141",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####5 Stock Table\n",
    "\n",
    "#importing dataset\n",
    "stock = DataFrame()\n",
    "stock = pd.read_csv(r\"C:\\Users\\yasin\\OneDrive\\Desktop\\BI\\CA2\\Raw data\\Stock_Dim.csv\")\n",
    "\n",
    "#renaming \"ProductKey\" to \"ProductID\" to \"SafetyStockQuantity\" to \"StockQuantity\"\n",
    "stock = stock.rename(columns={\"ProductKey\": \"ProductID\", \"SafetyStockQuantity\": \"StockQuantity\"})\n",
    "\n",
    "#dropping rows where \"ProductKey\" value <= 2517 in order to match the products list in our Products table\n",
    "stock.drop(stock[stock['ProductID'] > 2517].index, inplace=True)\n",
    "\n",
    "#grouping by \"ProductKey\" and aggregating \"StockQuantity\" by summing and \"UnitCost\" by mean value\n",
    "stock = stock.groupby('ProductID', as_index=False).agg({\n",
    "    'StockQuantity': 'sum',\n",
    "    'UnitCost': 'mean'  # or 'first' is similar results\n",
    "})\n",
    "\n",
    "#adding StockID in the table as primary key\n",
    "stock.insert(0, 'StockID', range(1, len(stock)+1))\n",
    "\n",
    "engine = create_engine('mysql+mysqlconnector://root:password@localhost/businessintelligence_warehouse')\n",
    "stock.to_sql(name='stock', con=engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c05849f-c376-4e20-a913-15d0be31ab79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2693e22c-952b-46d4-83f0-6a5f3dc0d2ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#replacing years in DateKey\\nexpenses['Datekey'] = expenses['Datekey'].apply(lambda x: x.replace(year={\\n    2007: 2022,\\n    2008: 2023,\\n    2009: 2024\\n}.get(x.year, x.year)))\\n\\n#droping unwanted columns\\nexpenses = expenses.drop(columns=['ETLLoadID', 'LoadDate', 'UpdateDate'])\\n\\n#exporting clean dataset to CSV file\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####6 Expenses Table\n",
    "\n",
    "expenses = pd.read_csv(r\"C:\\Users\\yasin\\OneDrive\\Desktop\\BI\\CA2\\Raw data\\Expenses_Dim.csv\")\n",
    "\n",
    "\n",
    "#parsing DateKey column to change from string to datetime value\n",
    "#expenses['Datekey'] = pd.to_datetime(expenses['Datekey'])\n",
    "\n",
    "#replacing years in DateKey\n",
    "expenses['Datekey'] = expenses['Datekey'].apply(lambda x: x.replace(year={\n",
    "    2007: 2022,\n",
    "    2008: 2023,\n",
    "    2009: 2024\n",
    "}.get(x.year, x.year)))\n",
    "\n",
    "#droping unwanted columns\n",
    "expenses = expenses.drop(columns=['ETLLoadID', 'LoadDate', 'UpdateDate'])\n",
    "\n",
    "#exporting clean dataset to CSV file\n",
    "\n",
    "\n",
    "engine = create_engine('mysql+mysqlconnector://root:password@localhost/businessintelligence_warehouse')\n",
    "expenses.to_sql(name='expenses', con=engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ebafd4-d5df-41bc-83da-04cf1d3c130b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
